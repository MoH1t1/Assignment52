{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "# Q1. What is the purpose of grid search CV in machine learning, and how does it work?\n\nGrid search cross-validation (CV) is used to find the best hyperparameters for a machine learning model by performing an exhaustive search over a predefined set of hyperparameters.\nIt evaluates every combination of parameters using cross-validation and selects the combination that results in the best model performance.\n\n# Q2. Describe the difference between grid search CV and randomized search CV, and when might you choose one over the other?\n    \nGrid Search CV: Evaluates all possible combinations of hyperparameters from a predefined grid, ensuring that the best combination is found, but can be computationally expensive.\nRandomized Search CV: Randomly samples a fixed number of hyperparameter combinations from a larger search space. It is more efficient and faster than grid search, especially when the search space is large.\nWhen to choose: Use grid search when you have a small, well-defined set of hyperparameters, and use randomized search when the search space is large, or you need faster results.\n\n# Q3. What is data leakage, and why is it a problem in machine learning? Provide an example.\n    \nData leakage occurs when information from outside the training dataset is used to create the model, leading to overly optimistic performance during training but poor generalization \non unseen data. Example: If future data (e.g., target variable values from future timestamps) is used to predict the outcome, the model will have access to information that\nwouldn't be available in real-world scenarios, leading to incorrect performance metrics.\n\n# Q4. How can you prevent data leakage when building a machine learning model?\n\nEnsure proper separation between training and test data.\nUse time-based splits for time-series data to avoid future data leakage.\nAvoid including features that directly depend on the target variable.\nUse cross-validation methods that prevent data leakage.\n    \n# Q5. What is a confusion matrix, and what does it tell you about the performance of a classification model?\n    \nA confusion matrix is a table that summarizes the performance of a classification model by comparing the predicted and actual labels. It shows the number of true positives,\nfalse positives, true negatives, and false negatives, helping to assess the accuracy and errors of the model.\n\n# Q6. Explain the difference between precision and recall in the context of a confusion matrix.\n                                                                                                                                                                           \nPrecision: The proportion of true positive predictions out of all positive predictions made by the model. It measures the accuracy of positive predictions.\nRecall: The proportion of true positive predictions out of all actual positive instances. It measures the model's ability to correctly identify positive instances.\n\n# Q7. How can you interpret a confusion matrix to determine which types of errors your model is making?\n\nFalse Positives (Type I Error): The model incorrectly predicts a positive class when it is actually negative.\nFalse Negatives (Type II Error): The model incorrectly predicts a negative class when it is actually positive. By analyzing the confusion matrix, we can determine if the model \n                                 is biased toward one class, whether it struggles with a specific class, or if it’s making systematic errors.\n\n# Q8. What are some common metrics that can be derived from a confusion matrix, and how are they calculated?\n\nAccuracy: (TP + TN)/(TP+TN+FP+FN)\nPrecision: TP/(TP+FP)\nRecall: TP/(TP+FN)\nF1-score: 2×(Precision×Recall)/(Precision+Recall)\nSpecificity: TN/(TN+FP) Where:\n  TP = True Positive\n  TN = True Negative\n  FP = False Positive\n  FN = False Negative\n    \n# Q9. What is the relationship between the accuracy of a model and the values in its confusion matrix?\n\nAccuracy is the proportion of correctly classified instances (both true positives and true negatives) over the total number of instances. It is calculated as:\n\n   Accuracy = TP+TN/TP+TN+FP+FN\n\nWhile accuracy is a useful metric, it can be misleading in imbalanced datasets, where the model might perform well by simply predicting the majority class.\n\nQ10. How can you use a confusion matrix to identify potential biases or limitations in your machine learning model?\n    \nA confusion matrix helps identify:\nBias toward a particular class: If there are many false negatives or false positives, the model might be biased toward predicting one class.\nImbalanced class prediction: If the model predicts the majority class too often, it might ignore the minority class.\nMisclassification types: By examining the false positives and false negatives, you can identify the types of errors the model makes and potentially adjust thresholds or improve data\n                         preprocessing.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}